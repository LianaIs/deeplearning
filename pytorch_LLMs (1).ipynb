{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda051a-3b7e-4f40-a6c7-e59759c5c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "review = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Load the pipeline for text classification\n",
    "classifier = pipeline(task=\"text-classification\", model=model_name)\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(review)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef2862-2fc5-45ae-b798-7a55409d458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length=50)\n",
    "\n",
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddde9d-47ac-469d-99dd-493d5e73febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "\n",
    "# Pass the inputs to the pipeline\n",
    "outputs = qa_model(context=context,question=question)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c6159-435e-4ec6-80a8-a541243e12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set transformer model hyperparameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864a494-15f8-48e5-8d3e-10d34e4d2aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d00d7-3514-44ad-8dde-b35c8490832d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95faa1b6-e0cb-4388-80f4-77af3969697b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330c4f9-6663-41b2-9af3-72a2ef90f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, query, key, value, mask=None):\n",
    "    batch_size = query.size(0)\n",
    "\n",
    "    query = self.split_heads(self.query_linear(query), batch_size)\n",
    "    key = self.split_heads(self.key_linear(key), batch_size)\n",
    "    value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "    attention_weights = self.compute_attention(query, key, mask)\n",
    "\t\t\n",
    "    # Multiply attention weights by values, concatenate and linearly project outputs\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "    return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7de07-1578-4dee-93ca-eb664b2392b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9b7ff-9b94-47f1-b7c9-ffa6e0c1a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635ad3b-aecc-4274-9876-4a2d8650422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aae0c-bb25-4cc7-ab40-7cc177f182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass \n",
    "output = encoder(input_sequence, mask)\n",
    "classification = classifier(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f9ca68-4cf4-4125-ba13-e88d7b77d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only transformer\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccb19f-1cda-4e10-9cd2-df9cc1adfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "\n",
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a31724-c81b-401d-be83-85063868c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporating cross-attention into Decoder\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002a368-7e7d-4a35-b825-32190efb60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8b6fc-b2a6-4618-8255-6cfe2c8fe927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize positional encoding layer and stack of EncoderLayer modules\n",
    "class TransformerEncoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the sequence through each layer in the encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Initialize the encoder stack of the Transformer\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457fe6c-1466-4cd0-b1b5-87c2d8854809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for classification\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ef22f-a631-4a4d-9af2-9f8a1208dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for text summarization\n",
    "\n",
    "print(f\"Number of instances: {len(dataset['train'])}\")\n",
    "\n",
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")\n",
    "\n",
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['reviews'][0]['review_text']\n",
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary = tokenizer.decode(\n",
    "  summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48976b1c-f4c0-4e8f-ac17-00f910eba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for translation\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    input_ids = tokenizer.encode(english_input,   return_tensors=\"pt\")\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c905da82-113a-4f84-960d-abd56b0e56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Who analyzed the biopsies?\n",
      "Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5f6771b85547f098e9dee4728ab66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ce8e4a59de4a8c85f58d6f5347da73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3870b2432bfc4cefa8bafb34a39c59d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4408f99a76a042c4a0e8f825598f4909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five encoded tokens:  tensor([  101,  2040, 16578,  1996, 16012])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
    "\n",
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a269d-6e17-412b-9226-aff86cb67668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb70bd8-d48b-4481-a66f-ba8f3583a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a437d-eb14-4e9a-ab38-b5303bef8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=topics_encoded[\"train_random\"],\n",
    "    \t\t\teval_dataset=topics_encoded[\"validation_random\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Training loop to fine-tune the model\n",
    "# trainer.train()\n",
    "\n",
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdaa9fd-f9c1-415d-9869-32e12daba994",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e96ba4c-204a-4979-8fe2-9003fcd64c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03d715e0d3446639cb971720cbb1bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [    {\"text\": \"I love this product!\", \"label\": 1},    \n",
    "                    {\"text\": \"The service was terrible.\", \"label\": 0},    \n",
    "                    {\"text\": \"This movie is amazing.\", \"label\": 1},    \n",
    "                    {\"text\": \"I'm disappointed with the quality.\", \"label\": 0},]\n",
    "\n",
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279c8b7-7b4d-44fe-93ff-172e1ea1a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182c682-a688-4dcb-91dd-ef6be241ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "956aa24f-fb05-418b-a08f-7171194dad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Latest research findings in Antarctica show \"pioneers\" such as Max Linder and Albert Breuer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80e45224c2446f69fc1275684b3a885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  76.7398910522461\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import evaluate\n",
    "\n",
    "# Load the text-generation pipeline and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "generator = pipeline(\"text-generation\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Latest research findings in Antarctica show\"\n",
    "\n",
    "# Generate text\n",
    "output = generator(prompt, max_length=20, num_return_sequences=1)\n",
    "\n",
    "# Extract generated text\n",
    "generated_text = output[0]['generated_text']\n",
    "\n",
    "print(\"Generated Text: \", generated_text)\n",
    "\n",
    "# Compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id='gpt2', predictions=[generated_text])\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03abf162-98d7-4933-ad49-a509d9d41e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Latest research findings in Antarctica show that the ice sheet is melting faster than previously thought.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add54d0aa30941a6a0577b55a5c88e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  2763.8544892051004\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# ... Load model and tokenizer ...\n",
    "prompt = \"Latest research findings in Antarctica show\"\n",
    "\n",
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(prompt_ids, max_length=20)\n",
    "generated_text = tokenizer.decode(\n",
    "  output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)\n",
    "\n",
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id='gpt2',\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011c35ec-bb04-4421-8c34-6ef4e5abf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rouge metric # need to install first\n",
    "rouge = evaluate.load(\"rouge\", module_type=\"metric\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions,                                   references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819f5f7-d1fb-497e-8fb7-e5d8d2d94a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results['meteor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a44bef-7e3f-415f-9007-266e1ccddc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(  references=references, predictions=predictions)\n",
    "print(\"EM results: \", results['exact_match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd286487-f9b5-4593-848b-d23088352a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1022259d7d64b248c98885046ecd04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8b74f4dbfd4a7e8b694395fc25fd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf302281ab4d43049ee24aa7b68743e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca0e1a1c4b0495683e6a2115154f09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m input_sentences_2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHola, ¿cómo estás?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstoy genial, gracias.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m references_2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m      [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m      [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm great, thanks.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm great, thank you.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m      ]\n\u001b[0;32m---> 14\u001b[0m translator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-es-en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Translate the first input sentence\u001b[39;00m\n\u001b[1;32m     17\u001b[0m translated_output \u001b[38;5;241m=\u001b[39m translator(input_sentence_1)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:994\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    992\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 994\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    995\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[1;32m    996\u001b[0m         )\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:913\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    914\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    915\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m             )\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)\n",
    "\n",
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "print(predictions)\n",
    "\n",
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0030ef-69ec-4507-b77c-8a0e08a86c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1134: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  last_non_masked_index = mask.nonzero()[-1]\n",
      "/opt/anaconda3/lib/python3.11/site-packages/trl/core.py:157: UserWarning: MPS: no support for int64 for sum_out_mps, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:155.)\n",
      "  return (values * mask).sum() / mask.sum()\n",
      "/opt/anaconda3/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1327: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "/opt/anaconda3/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1330: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import respond_to_batch\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "# Instantiate a reference model\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Initialize trainer configuration\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1)\n",
    "\n",
    "prompt = \"Next year, I \"\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "response  = respond_to_batch(model, input)\n",
    "\n",
    "# Create a PPOTrainer instance\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "reward = [torch.tensor(1.0)]\n",
    "\n",
    "# Train LLM for one step with PPO\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b1fe836-c5b1-49c9-901c-7f84e06228b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective/kl': 0.0,\n",
       " 'objective/kl_dist': 0.0,\n",
       " 'objective/logprobs': array([[-10.803115 , -10.834714 , -10.795106 , -10.808006 , -10.834154 ,\n",
       "         -10.784305 , -10.851145 , -10.829018 , -10.844852 , -10.80966  ,\n",
       "         -10.798936 , -10.829404 , -10.816059 , -10.8558445, -10.883343 ,\n",
       "         -10.77611  , -10.780057 , -10.800485 , -10.81772  , -10.809915 ,\n",
       "         -10.813437 , -10.853871 , -10.834602 , -10.838813 ]],\n",
       "       dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-10.803115 , -10.834714 , -10.795106 , -10.808006 , -10.834154 ,\n",
       "         -10.784305 , -10.851145 , -10.829018 , -10.844852 , -10.80966  ,\n",
       "         -10.798936 , -10.829404 , -10.816059 , -10.8558445, -10.883343 ,\n",
       "         -10.77611  , -10.780057 , -10.800485 , -10.81772  , -10.809915 ,\n",
       "         -10.813437 , -10.853871 , -10.834602 , -10.838813 ]],\n",
       "       dtype=float32),\n",
       " 'objective/kl_coef': 0.2,\n",
       " 'objective/entropy': 216.46173095703125,\n",
       " 'ppo/mean_non_score_reward': 0.0,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 5.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 5.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': -4.052333679283038e-05,\n",
       " 'ppo/loss/value': 0.7367329597473145,\n",
       " 'ppo/loss/total': 0.07363277673721313,\n",
       " 'ppo/policy/entropy': 10.824566841125488,\n",
       " 'ppo/policy/approxkl': 3.992161268229211e-08,\n",
       " 'ppo/policy/policykl': 4.110336521989666e-05,\n",
       " 'ppo/policy/clipfrac': 0.0,\n",
       " 'ppo/policy/advantages': array([-2.50642   , -2.517412  , -2.5289822 , -2.5411618 , -1.8115588 ,\n",
       "        -0.44298232, -0.35959473, -1.8542011 , -0.22813044, -1.508221  ,\n",
       "        -0.08674662,  0.04571644,  0.06058764,  0.2817816 , -1.2246319 ,\n",
       "         0.4887704 ,  0.6324431 ,  0.7843804 ,  0.8030062 ,  1.1094228 ,\n",
       "         1.2862531 , -0.17072491,  0.45911098,  1.7353148 , -2.50642   ,\n",
       "        -2.517412  , -2.5289822 , -2.5411618 , -1.8115588 , -0.44298232,\n",
       "        -0.35959473, -1.8542011 , -0.22813044, -1.508221  , -0.08674662,\n",
       "         0.04571644,  0.06058764,  0.2817816 , -1.2246319 ,  0.4887704 ,\n",
       "         0.6324431 ,  0.7843804 ,  0.8030062 ,  1.1094228 ,  1.2862531 ,\n",
       "        -0.17072491,  0.45911098,  1.7353148 , -2.50642   , -2.517412  ,\n",
       "        -2.5289822 , -2.5411618 , -1.8115588 , -0.44298232, -0.35959473,\n",
       "        -1.8542011 , -0.22813044, -1.508221  , -0.08674662,  0.04571644,\n",
       "         0.06058764,  0.2817816 , -1.2246319 ,  0.4887704 ,  0.6324431 ,\n",
       "         0.7843804 ,  0.8030062 ,  1.1094228 ,  1.2862531 , -0.17072491,\n",
       "         0.45911098,  1.7353148 , -2.50642   , -2.517412  , -2.5289822 ,\n",
       "        -2.5411618 , -1.8115588 , -0.44298232, -0.35959473, -1.8542011 ,\n",
       "        -0.22813044, -1.508221  , -0.08674662,  0.04571644,  0.06058764,\n",
       "         0.2817816 , -1.2246319 ,  0.4887704 ,  0.6324431 ,  0.7843804 ,\n",
       "         0.8030062 ,  1.1094228 ,  1.2862531 , -0.17072491,  0.45911098,\n",
       "         1.7353148 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': -2.2947789091176674e-07,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.9999981 ,\n",
       "        1.0000019 , 1.000063  , 1.000001  , 0.9997549 , 0.99999905,\n",
       "        1.0000067 , 0.9999428 , 1.0000563 , 1.0000563 , 0.99999523,\n",
       "        1.000001  , 0.9999666 , 1.000061  , 0.9999428 , 1.0000563 ,\n",
       "        1.0000544 , 0.9999981 , 1.0000029 , 1.0000534 , 1.0000553 ,\n",
       "        0.99999905, 0.99937934, 1.0000572 , 0.9999981 , 1.0000029 ,\n",
       "        1.000124  , 1.000001  , 0.99951756, 0.99999714, 1.0000134 ,\n",
       "        0.9998846 , 1.0001135 , 1.0001097 , 0.9999914 , 1.0000029 ,\n",
       "        0.99993134, 1.0001221 , 0.99988556, 1.0001116 , 1.0001106 ,\n",
       "        0.99999714, 1.0000029 , 1.0001068 , 1.0001106 , 0.99999714,\n",
       "        0.99877906, 1.0001135 , 0.9999962 , 1.0000038 , 1.0001831 ,\n",
       "        1.0000019 , 0.99928784, 0.9999962 , 1.000021  , 0.9998255 ,\n",
       "        1.0001698 , 1.0001602 , 0.9999857 , 1.0000038 , 0.99989414,\n",
       "        1.0001831 , 0.99982834, 1.0001669 , 1.0001669 , 0.99999523,\n",
       "        0.99999905, 1.0001612 , 1.0001669 , 0.9999962 , 0.9982001 ,\n",
       "        1.0001698 ], dtype=float32),\n",
       " 'ppo/returns/mean': 0.36477920413017273,\n",
       " 'ppo/returns/var': 0.12060604244470596,\n",
       " 'ppo/val/vpred': -0.7522729635238647,\n",
       " 'ppo/val/error': 1.473465919494629,\n",
       " 'ppo/val/clipfrac': 0.0,\n",
       " 'ppo/val/mean': -0.753814160823822,\n",
       " 'ppo/val/var': 0.10113444924354553,\n",
       " 'ppo/val/var_explained': -11.217181205749512,\n",
       " 'ppo/learning_rate': 1.41e-05,\n",
       " 'time/ppo/forward_pass': 5.666317701339722,\n",
       " 'time/ppo/compute_rewards': 0.3504650592803955,\n",
       " 'time/ppo/compute_advantages': 1.264526128768921,\n",
       " 'time/ppo/optimize_step': 7.521406888961792,\n",
       " 'time/ppo/calc_stats': 1.5534157752990723,\n",
       " 'time/ppo/total': 16.35629677772522}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da24f5b3-5967-4774-a43c-07e2e1a7ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3499af9f31d4b36b8aa23e88c034e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db66b02c1584c70b622c694184bfa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a63905bd4034ec889da89432c8d0bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ce6c33c34441d78ad3c47c1085e6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107e6196e9bb4383891913406faee40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3a9530d58643d5ae05915d0c05afda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1ccc93d2eb4a479743af9371b1c173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26467450b96244719e50c4526c541717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df2134094034e8da5f64d6ce660419a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eb28afe384497d8ff0b16eb766f8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff9cc0ba10349fe883feb1cf5adc6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7f8be2090d4278821f5a5767dd3f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f1106c6ddb42f189d5544c875825db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicities (emp. 1): [0.0001386617950629443, 0.00013368602958507836]\n",
      "Toxicities (emp. 2):  [0.00014245195779949427, 0.010071290656924248]\n",
      "Maximum toxicity (emp. 1): 0.0001386617950629443\n",
      "Maximum toxicity (emp. 2):  0.010071290656924248\n",
      "Toxicity ratio (emp. 1): 0.0\n",
      "Toxicity ratio (emp. 2):  0.0\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "toxicity_metric = load(\"toxicity\")\n",
    "regard = load(\"regard\")\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]\n",
    "\n",
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])\n",
    "\n",
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])\n",
    "\n",
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "119d0bf4-9e7f-46df-8f05-0774eab238a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity in group 1:\n",
      " {'regard': [[{'label': 'positive', 'score': 0.9098385572433472}, {'label': 'neutral', 'score': 0.059397004544734955}, {'label': 'other', 'score': 0.026468127965927124}, {'label': 'negative', 'score': 0.0042962608858942986}], [{'label': 'positive', 'score': 0.7809810638427734}, {'label': 'neutral', 'score': 0.1808600127696991}, {'label': 'other', 'score': 0.030492965131998062}, {'label': 'negative', 'score': 0.007666024845093489}]]}\n",
      "Polarity in group 2:\n",
      " {'regard': [[{'label': 'negative', 'score': 0.9658734202384949}, {'label': 'other', 'score': 0.02155590057373047}, {'label': 'neutral', 'score': 0.012026485055685043}, {'label': 'positive', 'score': 0.0005441230605356395}], [{'label': 'negative', 'score': 0.9774737358093262}, {'label': 'other', 'score': 0.012994571588933468}, {'label': 'neutral', 'score': 0.008945498615503311}, {'label': 'positive', 'score': 0.0005862839752808213}]]}\n",
      "Polarity comparison between groups:\n",
      " {'regard_difference': {'positive': 0.8448446070251521, 'neutral': 0.10964251682162285, 'other': 0.011205310467630625, 'negative': -0.9656924351584166}}\n"
     ]
    }
   ],
   "source": [
    "# Load the regard and regard-comparison metrics\n",
    "regard = load(\"regard\")\n",
    "regard_comp = load(\"regard\", \"compare\")\n",
    "\n",
    "\n",
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']\n",
    "\n",
    "\n",
    "# Compute the regard (polarities) of each group separately\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n",
    "\n",
    "# Compute the relative regard between the two groups for comparison\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
